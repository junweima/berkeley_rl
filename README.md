# Tranfer Learning notes:

## "forward" transfer (train on one task, transfer to a new task)
	a) try it and hope for the best
	b) architectures for transfer: progressive networks
		- how to train source task: finetune with maximum entropy (soft Qlearning)
		- what architecture to use for transfer: progressive networks
	c) finetune on new task
	d) randomize source task domain (potential improvements for Taylor)
		- randomizing physical parameters (EPOpt: learning robust neural netowrk policies Rajeswaran et al.) etc
	
### What if we can peek at target domain?
	- better transfer through domain adaptation - confusion loss
	
	
## multi-task transfer (train on many tasks, tranfer to a new task)





## multi-task meta-learning (learn to learn from many tasks)




	